---
layout: article
title: CS231n系列—神经网络中的优化方法
date: 2019-10-15 18:27:52
categories: 
- 学习笔记
tags: 
- 机器学习
type: "picture"
mathjax: true
---
CS231n看过已有半年多之久，当时在ipad上手写了不少笔记，花费不少时间。昨日看论文时翻看复习了优化方法这一部分，突然想起差不多也该整理整理笔记。一方面是因为这门课确实质量高，另一方面也希望在整理的过程中把这些知识温习一遍。<br>
本文作为学习笔记的第一篇，将常见的优化方法梳理一遍。
<!-- more -->

因为梯度下降（gradient descent）太基础了，严格来讲应该属于高数的内容，故直接从SGD开始。

# 1. SGD
随机梯度下降（SGD）相比于朴素的梯度下降，最大的差别就是多了“随机”两个字……先别打我，的确是这样，“随机”体现在每一轮用来更新参数的数据只是从整体数据集中“随机”挑选出的一部分。<br>
```
while True:
    weight_grad = evaluate_grad(loss_function, data, weights)
    weights += -stepsize * weight_grad
```
但SGD有三个问题：

- (1).条件数问题<br>
    如果loss在一个方向上变化过快，而在另一个方向上变化缓慢，那么就会表现出图中锯齿状更新路径：
    <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/cs231n/1_optimization/opt_1.png" width=50%>
    这还只是2个参数的情形。当有上百万个参数时，最大和最小的参数变化率的差异巨大，使得SGD的表现并不好。
- (2).陷入鞍点<br>
    当只有几个参数时，很容易理解陷入鞍点的情形：即各个参数在该点处的**一阶导数为零且二阶导数同号**。此时虽然一阶导数为零，优化停止，但明显连局部最优都算不上<br>
    但当参数量是上百万时，鞍点会成为一个大问题。<br>
    以10个参数为例，当某点是各个参数的驻点时，它是局部最优点的概率为$2×0.5^{10}$，而它是鞍点的概率则是$1-2×0.5^{10}$！也就是说，当用SGD优化停止时（一阶导数为0，更新停止），参数更有可能陷入到了鞍点而非局部最优点。<br>
    此外问题还会出现在鞍点附近，在接近鞍点的地方梯度会变得非常小，更新会非常缓慢。
- (3).“随机”<br>
    理想的情况是将所有样本一次性输入计算loss，但由于样本总量太大，这样做并不现实。故输入的是一个mini batch的样本，但这意味着得到的并非真正的梯度，而是对真正梯度的一个估计（课程中称之为noisy gradient），所以更新会变得缓慢。

幸运的是，SGD+momentum可以解决上述问题。

# 2. SGD+momentum
$$ v_{t+1}=\rho v_{t}-\nabla f(x_{t}) $$
$$ x_{t+1}=x_{t}+\alpha v_{t+1} $$
此时不再沿梯度方向更新，而是沿速度方向更新，其中$\rho$是衰减因子。
它的好处在于：

- (1).对于条件数问题，在没有引入速度前，更新路径为锯齿状；但有了速度之后，就有可能抵消各个参数间的变化差异。这里的解释是，梯度变化敏感的那个方向上因为有了上一次的速度，“惯性”会使得其“调头”变得困难；同时，梯度变化迟缓的那个方向上则会一直加速。
- (2).对于陷入鞍点的问题，当遇到局部最优点或鞍点时，由于具有上一次的“速度”，即使此处的梯度为零，仍可以通过顺利通过。
- (3)关于noisy gradient的问题，由于引入了速度，使更新路径更加平缓，这在一定程度上缓解了“噪声”。此外，从更新公式中可以看出，速度不断地衰减并与梯度累加（初始速度为零），所以不妨把速度看做是之前所有梯度（各个方向上）的折衷，故更新路径更加平滑。

# 3. Nesterov Momentum
$$ v_{t+1}=\rho v_{t}+\alpha \nabla f(x_{t}+\rho v_{t}) $$
$$ x_{t+1}=x_{t}+v_{t+1} $$
Nesterov Momentum与SGD+momentum相比最大区别在于，计算gradient的位置$(x_t+\rho v_t)$和实际更新的位置$(x_t)$不同。这么操作乍一看难免让人有点难以理解，为此我们做以下变换：<br>

$$
令\quad \tilde{x}=x_t+\rho v_t \xrightarrow{代入}
\left\{
\begin{split}
v_{t+1}&=\rho v_t-\alpha \nabla f(\tilde{x_t})\\
x_{t+1}&=\tilde{x_t}-\rho v_t+(1+\rho)v_{t+1}\\
&\tilde{x_t}+v_{t+1}+\rho (v_{t+1}-v_t)
\end{split}
\right .
$$
如此一来，Nesterov Momentum就有了良好的解释——即相比于SGD+momentum只是多了一项“速度差”。<br>
定性的理解是：我们引入“速度”的目的是使优化过程可以顺利地通过驻点，也就是故意跑偏；但我们又不希望这个“速度”使更新方向跑偏得太离谱，毕竟计算出的（负）梯度方向才是下降最快的方向，最好的情况是在计算出的更新方向附近跑偏一点点就好。
从矢量图中可知，引入了“速度差”之后，实际的更新方向会更加偏向负梯度方向，于是“速度”的影响就被削弱了。<br>
当然，Nesterov Momentum在凸优化问题中还有一些其它良好的理论性质。
---
layout: article
title: 对抗学习-Chapter3：对抗样本&求解内部最大化问题
date: 2020-09-10 18:54:33
categories: 
- 学习笔记
tags: 
- 机器学习
comment: true
type: "picture"
key: Adversarial-Robustness
---

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/adv_examples.svg" /></div>

<!--more-->

这次我们来考虑神经网络的情况。

再一次回到那个优化问题，内部最大化问题


$$
\DeclareMathOperator*{\maximize}{maximize}
\maximize_{\|\delta\| \leq \epsilon} \ell(h_\theta(x), y)
$$


中的$$h_\theta(x)$$此时表示的是神经网络。为了进一步研究，我们需要给出神经网络$$h_\theta(x)$$更准确的定义。用如下等式来定义一个$d$层的网络$$h_\theta(x):\mathbb{R}^n \rightarrow \mathbb{R}^k$$：


$$
% <![CDATA[
\begin{split}
z_1 & = x \\
z_{i+1} & = f_i(W_i z_i + b_i), \;\; i,\ldots,d \\
h_\theta(x) & = z_{d+1}
\end{split} %]]>
$$


其中$z_i$表示第$i$层的输出；$f_i$表示第$i$层的激活函数，通常前$d-1$层使用RELU $$f_i(z) = \max\{0,z\}$$，第$d$层使用线性激活函数$f_i(z) = z$。网络的参数是$$\theta = \{W_1,b_1,\ldots,W_d,b_d\}$$。损失函数采用之前使用的多分类交叉熵损失：


$$
\ell(h_\theta(x), y) = \log \left ( \sum_{j=1}^k \exp(h_\theta(x)_j) \right ) - h_\theta(x)_y.
$$


和线性模型相比，神经网络损失函数的解空间（按文章中直译的话应该叫做损失平面 loss surface）会更加“崎岖”。换句话说就是我们上一章提到的那条“分界线”会更“崎岖”。作者随便训练了一个神经网络来说明这个问题：

```python
import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(0)
model = nn.Sequential(nn.Linear(1,100), nn.ReLU(), 
                      nn.Linear(100,100), nn.ReLU(), 
                      nn.Linear(100,100), nn.ReLU(), 
                      nn.Linear(100,1))
opt = optim.SGD(model.parameters(),lr=1e-2)
for _ in range(100):
    loss = nn.MSELoss()(model(torch.randn(100,1)), torch.randn(100,1))
    opt.zero_grad()
    loss.backward()
    opt.step()
                      
plt.plot(np.arange(-3,3,0.01), model(torch.arange(-3,3,0.01)[:,None]).detach().numpy())
plt.xlabel("Input")
plt.ylabel("Output")
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_0.svg" /></div>

这会带来两个主要的挑战。

第一，在我们通常使用深度神经网络这种高维情况中，很有可能在输入空间的任何一个点都存在在一些方向上损失平面会非常陡峭，也就是说会造成损失大幅增加或减少。这正是我们在Chapter 1中见到的那样，对输入做很小的扰动就会导致损失大幅增加。换句话说，*神经网络由于其损失平面的性质，特别容易出现对抗性样本*。

第二，不像线性模型，想要求解内部最大化问题并不容易。正如上图所示，神经网络的损失平面是非凸的、有很多极值点（这里只考虑针对输入，而不是针对参数）。当我们尝试最大化或最小化上述函数时，给定点处的初始梯度可能会或可能不会指向全局最值的方向。第二点在我们试图生成一些对抗样本的时候并不会构成问题，毕竟正如第一点所说，有很多方向都可以导致损失大幅增加，沿着这些方向就可以找到一些还不错的对抗样本，即使它不是最优的那个对抗样本。但是，当我们想要去进行稳健性优化时，就会发现出了大问题，因为Danskin’s theorem不再成立了，进而无法去求解真正的稳健性优化问题。

## 求解内部最大化问题的策略

所以该如何（近似地）求解内部优化问题


$$
\maximize_{\|\delta\| \leq \epsilon} \ell(h_\theta(x), y)
$$


当假设函数$$h_\theta$$是神经网络？有三种主要的策略来解决，分别对应着*下界，精确解*和*上界*。

具体的，我们有如下三种选择：

1. 我们可以找到优化目标的下界。因为任何可行的$\delta$都会给出一个下界，也就是说可以“经验地了来求解优化问题”，即找到一个对抗样本。我对此的理解是，既然算不出就强行找出来，利用像SGD这种算法，也就是Chapter 1中所做的那样。这也是目前最常用的求解内部最大化问题的策略，它背后的理由是，神经网络的局部极值问题似乎并不如我们想象的那么严重。然而，为了找到足够好的对抗样本并用其来训练稳健的模型，需要我们足够好地来求解这个问题。
2. 我们可以精确地求解这个优化问题。这当然很有挑战性，但其实对于许多激活函数而言，我们都可以将最大化问题形式化为一个组合优化问题，并通过混合整数规划（mixed integer programming）这样的技巧来精确求解。虽然这些方法在扩展到大型模型时会面临巨大挑战，但是对于小的问题，它们强调了一个重要的观点，即在某些情况下可以构造出内部最大化问题的精确解。
3. 最后可以找到优化目标的上界，和我们在多类别线性分类中所做的类似（这部分作者还没写）。其基本策略是考虑一个*松弛*的网络结构，使得这个松弛版本的网络既包含原本的网络，而且其构建方式更易于精确地优化。这些方法有些不同的是，因为它们通常不会为真实的网络构建实际的对抗样本（因为它们所操作的是松弛的模型，不同于原始模型），但它们可以证明该网络具有强大的抵抗攻击的能力。此外，当用于稳健性优化时，这些方法的效果是最好的。

## 一些神经网络示例

作者训练了三个简单的神经网络来作为后续实验的测试平台。这三个神经网络分别是：两层和四层的全连接网络、四层卷积层加一层全连接层的卷积神经网络。

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.shape[0], -1)    

model_dnn_2 = nn.Sequential(Flatten(), nn.Linear(784,200), nn.ReLU(), 
                            nn.Linear(200,10)).to(device)

model_dnn_4 = nn.Sequential(Flatten(), nn.Linear(784,200), nn.ReLU(), 
                            nn.Linear(200,100), nn.ReLU(),
                            nn.Linear(100,100), nn.ReLU(),
                            nn.Linear(100,10)).to(device)

model_cnn = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),
                          nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),
                          nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
                          nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),
                          Flatten(),
                          nn.Linear(7*7*64, 100), nn.ReLU(),
                          nn.Linear(100, 10)).to(device)
```

 训练过程的代码就不放在这里，参考 [Chapter 3](https://adversarial-ml-tutorial.org/adversarial_examples/)。

这些模型将会在后续用于试验不同的求解内部问题的策略。

## 内部最大化的下界

这也许是求解$$\DeclareMathOperator*{\maximize}{maximize} \maximize_{\|\delta\| \leq \epsilon} \ell(h_\theta(x + \delta), y)$$最简单直接的方法了。做法也很基本，就是反向传播。通过计算损失函数关于$\delta$的梯度，不断让梯度下降从而最大化目标函数。当然，这里我们需要保证$\delta$满足范数的约束，因此可以在每一次更新后都将$\delta$再重新投射到约束范围内。

### 快速梯度下降（Fast Gradient Sign Method）

我们首先计算出调整$\delta$的方向：


$$
g := \nabla_\delta \ell(h_\theta(x + \delta),y)
$$


如果计算的梯度是在$\delta=0$时（也就是第一次更新），那么它实际上等价于$$\nabla_x \ell(h_\theta(x),y)$$，只是我们为了表述的一致性依然称之为关于$\delta$的梯度。

为了最大化损失，需要让$\delta$在梯度下降的方向上以一定的步长$\alpha$迈出一步（take a step）：


$$
\delta := \delta + \alpha g
$$


当然还需要保证更新后的$\delta$在约束范围内$$\|\delta\| \leq \epsilon$$。这里我们取无穷范数$$\ell_\infty$$.

现在的问题是，步长应该取多大？如果我们想要让损失尽可能大的话，就应该取尽可能大的$\alpha$。如果取了一个很大的$\alpha$，不仅会被重新投射回$[-\epsilon, \epsilon]$内，而且$g$对更新的影响就只剩下决定符号正负了（$g$的大小相较于$\alpha$变得不再重要）。于是，当我们取一个很大的$\alpha$时，更新就为


$$
\delta := \epsilon \cdot \mathrm{sign}(g).
$$


这就是所谓的快速梯度下降法（FGSM），它是深度学习社区最早提出的构造对抗样本的方法之一。

其代码如下：

```python
def fgsm(model, X, y, epsilon):
    """ Construct FGSM adversarial examples on the examples X"""
    delta = torch.zeros_like(X, requires_grad=True)
    loss = nn.CrossEntropyLoss()(model(X + delta), y)
    loss.backward()
    return epsilon * delta.grad.detach().sign()
```

我们来看看构造的效果如何。

```python
for X,y in test_loader:
    X,y = X.to(device), y.to(device)
    break
    
def plot_images(X,y,yp,M,N):
    f,ax = plt.subplots(M,N, sharex=True, sharey=True, figsize=(N,M*1.3))
    for i in range(M):
        for j in range(N):
            ax[i][j].imshow(1-X[i*N+j][0].cpu().numpy(), cmap="gray")
            title = ax[i][j].set_title("Pred: {}".format(yp[i*N+j].max(dim=0)[1]))
            plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))
            ax[i][j].set_axis_off()
    plt.tight_layout()
```

```python
### Illustrate original predictions
yp = model_dnn_2(X)
plot_images(X, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_1.svg" /></div>

```python
### Illustrate attacked images
delta = fgsm(model_dnn_2, X, y, 0.1)
yp = model_dnn_2(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_2.svg" /></div>

仅仅是做了一些微小的变动，预测结果就从原来的只错了一个case变成只对了一个case。而这种变动对于人眼的判断并无影响。值得指出的是，全连接网络对这个问题是非常敏感的，相比之下，卷积网络的情况能好那么一些（当然还是很敏感）。我们来看看卷积神经网络上的表现如何。

```python
### Illustrate attacked images
delta = fgsm(model_cnn, X, y, 0.1)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_3.svg" /></div>

直观上来看上面的对抗样本效果很不错，但是让我们来更加严格地评估攻击方法的性能——测试误差。

```python
def epoch_adversarial(model, loader, attack, *args):
    total_loss, total_err = 0.,0.
    for X,y in loader:
        X,y = X.to(device), y.to(device)
        delta = attack(model, X, y, *args)
        yp = model(X+delta)
        loss = nn.CrossEntropyLoss()(yp,y)
        
        total_err += (yp.max(dim=1)[1] != y).sum().item()
        total_loss += loss.item() * X.shape[0]
    return total_err / len(loader.dataset), total_loss / len(loader.dataset)
```

```python
print("2-layer DNN:", epoch_adversarial(model_dnn_2, test_loader, fgsm, 0.1)[0])
print("4-layer DNN:", epoch_adversarial(model_dnn_4, test_loader, fgsm, 0.1)[0])
print("        CNN:", epoch_adversarial(model_cnn, test_loader, fgsm, 0.1)[0])
```

```
2-layer DNN: 0.9259
4-layer DNN: 0.8827
        CNN: 0.4173
```

在继续之前，关于FGSM有几点需要说明。

第一，FGSM是针对无穷范数$$\ell_\infty$$下的攻击所设计的，因为FGSM只是在$$\ell_\infty$$约束下的一步更新。不过也可以容易地将其推广到其它范数情况下，后续会进一步说明。

第二，FGSM基于的假设是，在$x$点处计算的梯度方向上的线性近似是函数在整个区域$$\|\delta\|*\infty \leq \epsilon$$内的一个合理的近似，但对于神经网络而言，事实并不如此，因为即使在一块很小的区域内损失平面也不是线性的。换句话说就是，在$x$这一点上的确算出了梯度下降的方向，但朝着整个方向走一步后，这个方向很大可能已经不再是最优的方向了。如果想要得到更好的对抗样本，这种仅做一步更新并投射的方法还远远不够。

### 投影梯度下降（Projected gradient descent）

上述讨论引入了下一种求解内部问题的方法：投影梯度下降。即和上述过程类似，只是从一步变成了迭代、从大步长变成了小步长。基本的PGD算法就是重复如下过程：


$$
% <![CDATA[
\begin{split}
& \mbox{Repeat:} \\
& \quad \delta := \mathcal{P}(\delta + \alpha \nabla_\delta \ell(h_\theta(x+\delta), y))
\end{split} %]]>
$$


其中$$\mathcal{P}$$表示约束至范围内的投影操作（例如针对无穷范数$$\ell_\infty$$的裁剪clip）。在PGD中可以指定诸如步长、迭代次数等。实际上Pytorch的优化器就可以完成这一工作，不过作者还是手工实现了如下代码，因为想要了解到底发生了什么。

```python
def pgd(model, X, y, epsilon, alpha, num_iter):
    """ Construct FGSM adversarial examples on the examples X"""
    delta = torch.zeros_like(X, requires_grad=True)
    for t in range(num_iter):
        loss = nn.CrossEntropyLoss()(model(X + delta), y)
        loss.backward()
        delta.data = (delta + X.shape[0]*alpha*delta.grad.data).clamp(-epsilon,epsilon)
        delta.grad.zero_()
    return delta.detach()
```

然后看看在卷积神经网络上的表现：

```python
### Illustrate attacked images
delta = pgd(model_cnn, X, y, 0.1, 1e4, 1000)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_4.svg" /></div>

对抗样本的效果似乎不如FGSM，甚至有个数字0的case看起来并没有加任何扰动。这主要是因为在$\delta=0$时的梯度太小了：

```python
delta = torch.zeros_like(X, requires_grad=True)
loss = nn.CrossEntropyLoss()(model_cnn(X + delta), y)
loss.backward()
print(delta.grad.abs().mean().item())
```

```
1.8276920172866085e-06
```

可以看到，在任意像素上的梯度绝对值的平均值只有$10^{-6}$。这说明在$\delta=0$周围的小区域内较为“平坦”，所以我们需要用相对较大的$\alpha$来突破这块平坦区域。

**附：最速下降（steepest descent）**。为了解决上述问题，这里采用（规范化）最速下降法。作者简单地介绍了最速下降法的概念。原本的梯度下降法是通过在梯度方向上反复迭代


$$
z := z - \alpha \nabla_z f(z).
$$


但问题是对梯度值的尺度很敏感，需要根据梯度值的尺度来调整步长。相反，规范化最速下降法则是在满足约束的步径范围内（范数约束）找到的能够最大化与梯度方向內积的方向$v$，也即求梯度方向的对偶范数。（然后在方向$v$上进行线搜索，这部分作者没提）


$$
\DeclareMathOperator*{\argmax}{argmax}
z := z - \argmax_{\|v\| \leq \alpha} v^T \nabla_z f(z).
$$


如果使用的$\ell_2$范数的话，该方向可以解析地求出（不过这里作者的推导中多一个比例因子$\alpha$，所以和[《Convex Optimization》](http://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)中的推导不太一样）


$$
\argmax_{\|v\|_2 \leq \alpha} v^T \nabla_z f(z) = \alpha \frac{\nabla_z f(z)}{\|\nabla_z f(z)\|_2}
$$


可以看出该方向是梯度方向单位化乘以比例因子（如果没有比例因子的话，该方向就是梯度方向，与书中结论一致）。同理，如果我们使用$\ell_\infty$来约束$v$，那么该方向为


$$
\argmax_{\|v\|_\infty \leq \alpha} v^T \nabla_z f(z) = \alpha \cdot \mathrm{sign}(\nabla_z f(z)).
$$



回到之前的神经网络对抗攻击上：迭代选择最速下降方向更新，相当于进行了多次“mini-FGSM”。这种方法被广泛称为“投影梯度下降”（投影体现在更新保证扰动在约束范围内）

```python
def pgd_linf(model, X, y, epsilon, alpha, num_iter):
    """ Construct FGSM adversarial examples on the examples X"""
    delta = torch.zeros_like(X, requires_grad=True)
    for t in range(num_iter):
        loss = nn.CrossEntropyLoss()(model(X + delta), y)
        loss.backward()
        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)
        delta.grad.zero_()
    return delta.detach()
```

```python
### Illustrate attacked images
delta = pgd_linf(model_cnn, X, y, epsilon=0.1, alpha=1e-2, num_iter=40)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_5.svg" /></div>

效果好多了。不仅效果比FGSM要好，而且选择步长也比之前要容易。因为步长$\alpha$和扰动总量$\epsilon$处于同一尺度，有理由选择$\alpha$为$\epsilon$的一个合理的一小部分，并迭代次数为$\epsilon/\alpha$的较小倍数（这句话没懂）。然后来看看这类攻击在整个测试集上的效果。

```python
print("2-layer DNN:", epoch_adversarial(model_dnn_2, test_loader, pgd_linf, 0.1, 1e-2, 40)[0])
print("4-layer DNN:", epoch_adversarial(model_dnn_4, test_loader, pgd_linf, 0.1, 1e-2, 40)[0])
print("CNN:", epoch_adversarial(model_cnn, test_loader, pgd_linf, 0.1, 1e-2, 40)[0])
```

```
2-layer DNN: 0.9637
4-layer DNN: 0.9838
CNN: 0.7432
```

相比于FGSM，有了合理的效果提升。

还有最后一种策略可以进一步微小提升效果：随机化（randomization）。这类技术实际中并不常用，因为代价比较大，但它强调了一个重点。

PGD的效果其实依然受目标函数局部最优点的限制。而这个问题可以通过多次随机选择不同的起点来缓解。这种微小的效果提升说明了从0开始计算PGD的确会陷入到一些局部最优点中。当然，不利的一面就是会成倍地增加程序的运行时间，在许多情况中都不太可行。

```python
def pgd_linf_rand(model, X, y, epsilon, alpha, num_iter, restarts):
    """ Construct PGD adversarial examples on the samples X, with random restarts"""
    max_loss = torch.zeros(y.shape[0]).to(y.device)
    max_delta = torch.zeros_like(X)
    
    for i in range(restarts):
        delta = torch.rand_like(X, requires_grad=True)
        delta.data = delta.data * 2 * epsilon - epsilon
        
        for t in range(num_iter):
            loss = nn.CrossEntropyLoss()(model(X + delta), y)
            loss.backward()
            delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)
            delta.grad.zero_()
        
        all_loss = nn.CrossEntropyLoss(reduction='none')(model(X+delta),y)
        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]
        max_loss = torch.max(max_loss, all_loss)
        
    return max_delta
```

```python
print("CNN:", epoch_adversarial(model_cnn, test_loader, pgd_linf_rand, 0.1, 1e-2, 40, 10)[0])
```

```
CNN: 0.7648
```

### 针对性攻击

上面的演示都是非针对性攻击，即只要把让模型把标签预测错误即可。但正如在第一章介绍中所述，我们也可以让模型错误地预测为我们期望的类别。


$$
\maximize_{\|\delta\| \leq \epsilon} \left ( \ell(h_\theta(x + \delta), y) - \ell(h_\theta(x + \delta), y_{\mathrm{targ}}) \right ) \equiv \maximize_{\|\delta\| \leq \epsilon} \left ( h_\theta(x + \delta)_{y_{\mathrm{targ}}} - h_\theta(x + \delta)_y \right )
$$



我们用PGD来进行攻击，这里为了使得目标类别可以是MNIST数据集中的大多数类别，使用了较大的扰动范围，$\epsilon=0.2$。

```python
def pgd_linf_targ(model, X, y, epsilon, alpha, num_iter, y_targ):
    """ Construct targeted adversarial examples on the examples X"""
    delta = torch.zeros_like(X, requires_grad=True)
    for t in range(num_iter):
        yp = model(X + delta)
        loss = (yp[:,y_targ] - yp.gather(1,y[:,None])[:,0]).sum()
        loss.backward()
        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)
        delta.grad.zero_()
    return delta.detach()
```

我们试图让模型预测为数字2：

```python
delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=2)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_6.svg" /></div>

结果相当好：只用了稍微大一些的$\epsilon$就能欺骗模型将所有被测样本预测为数字2（数字2本身并没有被预测错，是因为在这个case上的损失是0）。我们在将目标类别改为数字0试试：

```python
delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=0)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_7.svg" /></div>

这一次尽管我们使模型在所有的非0数字上都预测错了，但却并没有让所有样本都被预测成目标类别。这是因为我们的最大化的目标是类别0对应的位（logit，理解成预测值即可）减去真实类别对应的位（这个量越大会使预测朝着数字0的预测值越大、真实类别的预测值越小的方向上优化）。但在这个过程中，我们并没有关注其他类别的位发生了什么变化。也许在某些样本上，其它类别的预测值要比数字0的预测值要大。我们可以通过修改目标函数，最大化目标类别预测值的同时，最小化其它类别的预测值：


$$
\maximize_{\|\delta\| \leq \epsilon} \left ( h_\theta(x + \delta)_{y_{\mathrm{targ}}} - \sum_{y' \neq y_{\mathrm{targ}}} h_\theta(x + \delta)_{y'} \right )
$$

```python

def pgd_linf_targ2(model, X, y, epsilon, alpha, num_iter, y_targ):
    """ Construct targeted adversarial examples on the examples X"""
    delta = torch.zeros_like(X, requires_grad=True)
    for t in range(num_iter):
        yp = model(X + delta)
        loss = 2*yp[:,y_targ].sum() - yp.sum()
        loss.backward()
        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)
        delta.grad.zero_()
    return delta.detach()
```

```python
delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=0)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_8.svg" /></div>

由于这个优化目标要困难的多，因此我们没有做到完全欺骗分类器。但在骗到分类器的样本上，大多数都被预测成为目标类别。

### 非$\ell_\infty$范数

截至目前，我们都在考虑对$\delta$的无穷范数$\ell_\infty$约束。在这一部分，我们终于可以考虑$\ell_2$范数（当然，依然需要额外地将$x+\delta$约束在$[0,1]$范围内）。我们使用投影后的最速下降法，有如下形式：


$$
\delta := \mathcal{P}_\epsilon \left(\delta - \alpha \frac{\nabla_\delta \ell(h_\theta(x+\delta),y)}{\|\nabla_\delta \ell(h_\theta(x+\delta),y)\|_2}\right )
$$


（不理解为什么要用$\delta$减的话，看下面的代码就明白了）

其中$\mathcal{P}_\epsilon$表示投影到半径为$\epsilon$的$\ell_2$球上的操作：


$$
\mathcal{P}_\epsilon(z) = \epsilon\frac{z}{\max\{\epsilon, \|z\|_2\}}.
$$


于是，最终的攻击如下所示：

```python
def norms(Z):
    """Compute norms over all but the first dimension"""
    return Z.view(Z.shape[0], -1).norm(dim=1)[:,None,None,None]


def pgd_l2(model, X, y, epsilon, alpha, num_iter):
    delta = torch.zeros_like(X, requires_grad=True)
    for t in range(num_iter):
        loss = nn.CrossEntropyLoss()(model(X + delta), y)
        loss.backward()
        delta.data += alpha*delta.grad.detach() / norms(delta.grad.detach())
        delta.data = torch.min(torch.max(delta.detach(), -X), 1-X) # clip X+delta to [0,1]
        delta.data *= epsilon / norms(delta.detach()).clamp(min=epsilon)
        delta.grad.zero_()
        
    return delta.detach()
```

```python
delta = pgd_l2(model_cnn, X, y, epsilon=2, alpha=0.1, num_iter=40)
yp = model_cnn(X + delta)
plot_images(X+delta, y, yp, 3, 6)
```

<div style="text-align: center"><img src="https://adversarial-ml-tutorial.org/adversarial_examples/output_9.svg" /></div>

```python
print("2-layer DNN:", epoch_adversarial(model_dnn_2, test_loader, pgd_l2, 2, 0.1, 40)[0])
print("4-layer DNN:", epoch_adversarial(model_dnn_4, test_loader, pgd_l2, 2, 0.1, 40)[0])
print("CNN:", epoch_adversarial(model_cnn, test_loader, pgd_l2, 2, 0.1, 40)[0])
```

```
2-layer DNN: 0.9232
4-layer DNN: 0.9435
CNN: 0.7854
```

需要注意的是，这里$\epsilon$的取值比在$\ell_\infty$的情况下要大，因为$\ell_2$球的体积是$\ell_\infty$的体积的$\sqrt{n}$倍，其中$n$是输入的维度（确切的讲，其实还有一个额外的比例因子$\sqrt{2}/\sqrt{\pi e}$以使体积在更高的尺寸上更精确地匹配）。所以对于$\ell_2$球若要有和$\epsilon=0.1$的$\ell_\infty$球相同的体积，近似等价于$\ell_2$球的半径为$\frac{\sqrt{2\cdot784}}{\sqrt{\pi e}} \cdot 0.1 \approx 1.35$，这里我们使用了稍大一点的半径$\epsilon=2$，因为从经验上讲，分类器能够处理比$\ell_\infty$扰动稍大的$\ell_2$扰动。

这个例子中的一个关键点是，$\ell_\infty$攻击导致的扰动几乎分布在整个图像上（这正是$\ell_\infty$球允许的扰动）；而$\ell_2$攻击造成的扰动发生在图像的局部。

### 小结

作者再一次吐槽了该领域中论文关于攻击方法起的各式各样的名字。其实核心就两点（作者的观点）：1.用的哪种扰动范数球；2.用什么方法优化该范数扰动。